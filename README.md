# ğŸ¤– Chatbot â€“ LangChain + Ollama + Streamlit

A fully local AI-powered chatbot built using **LangChain**, **LCEL**, and **Ollama** â€” featuring message history, configurable models, and a clean **Streamlit UI**.

Supports open-source models like **Gemma 3**, **Phi 3**, and **Mistral 7B**, running completely offline on your machine.

---

## ğŸš€ Features
- ğŸ§  Session-aware chat using `RunnableWithMessageHistory`
- âš¡ Local inference via Ollama (no API key needed)
- ğŸ’¬ Streamlit frontend with dynamic message updates
- ğŸ”„ Configurable model via `.env` file
- ğŸª¶ Lightweight and modular 2-file design

---

## ğŸ§© Project Structure
```
chatbot_backend.py     # LangChain backend with Ollama + memory
chatbot_frontend.py    # Streamlit chat interface
.env                   # Environment variables (ignored in Git)
requirements.txt        # Python dependencies
.gitignore              # Ignore venv, .env, cache, etc.
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository
```
git clone https://github.com/manojmk04/ChatBot.git
cd ChatBot
```

### 2ï¸âƒ£ Create a virtual environment
```
python -m venv venv
venv\Scripts\activate  # On Windows
# or
source venv/bin/activate  # On macOS/Linux
```

### 3ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```

### 4ï¸âƒ£ Create `.env` file
```
OLLAMA_MODEL=phi3:3.8b
```

### 5ï¸âƒ£ Start Ollama
```
ollama serve
```

*(Pull the model first if not already installed:)*
```
ollama pull phi3:3.8b
```

### 6ï¸âƒ£ Run the chatbot
```
streamlit run chatbot_frontend.py
```

Then open the local URL shown (usually `http://localhost:8501`).

---

## ğŸ§  Switching Models
You can change the model anytime by editing your `.env` file:

```
OLLAMA_MODEL=gemma3:1b
# or
OLLAMA_MODEL=mistral:7b
# or
OLLAMA_MODEL=phi3:3.8b
```

Then rerun Streamlit to load the new model.

---

## ğŸ§¹ Clear Chat History
Click the â€œğŸ§¹ Clear Chatâ€ button (if present in UI) or restart Streamlit to start a fresh session.

---
